{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 20, 'total_tokens': 24, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'id': 'chatcmpl-BD6aHbGGUl0yu57oWJf1BugRdr8Lw', 'finish_reason': 'stop', 'logprobs': None}, id='run-e56e526c-1323-403b-ac45-eab42e6ea6d2-0', usage_metadata={'input_tokens': 20, 'output_tokens': 4, 'total_tokens': 24, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao!\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|C|iao|!||"
     ]
    }
   ],
   "source": [
    "for token in model.stream(messages):\n",
    "    print(token.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"deepseek-r1:8b\",\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"You are a helpful assistant that translates English to French. Translate the user sentence.\"),\n",
    "    HumanMessage(\"I love programming.\"),\n",
    "]\n",
    "\n",
    "# ai_message = llm.invoke(messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out how to translate \"I love programming.\" into French. Let me start by breaking down each part of the sentence.\n",
      "\n",
      "First, \"I love\" is straightforward. In English, it's \"I love,\" which in French would be \"J'adore.\" That makes sense because \"j'adore\" is a common way to express deep affection or enjoyment for something.\n",
      "\n",
      "Next, the word \"programming.\" In French, programming is called \"le d√©veloppement de logiciels\" or more commonly just \"le programmation.\" So, I can use either of those. Since it's about programming itself rather than software development specifically, maybe \"le programmation\" is better because it directly translates to \"programming.\"\n",
      "\n",
      "Putting it all together, the sentence would be \"J'adore le programmation.\" That sounds natural and conveys the same sentiment as the original English statement.\n",
      "\n",
      "I should also consider if there's a more colloquial way to say it. Sometimes people might use \"Je aime\" instead of \"J'adore,\" but \"j'adore\" is stronger and more expressive, which fits better when talking about something you really enjoy, like programming.\n",
      "\n",
      "Another thing to think about is the context. If someone is talking about their passion for coding or software development, using \"le programmation\" is appropriate because it's a common term in French for programming. There's no need to overcomplicate it with more specific terms unless the context requires it.\n",
      "\n",
      "I don't see any grammatical issues with this translation. The structure follows the same sentence pattern as English, making it easy for a French speaker to understand and relate to.\n",
      "\n",
      "So, after considering all these points, I'm confident that \"J'adore le programmation\" is the correct and natural way to translate \"I love programming.\" in French.\n",
      "</think>\n",
      "\n",
      "The translation of \"I love programming.\" into French is \"J'adore le programmation.\" This phrase effectively conveys a deep passion for programming in a natural and colloquial manner."
     ]
    }
   ],
   "source": [
    "for token in llm.stream(messages):\n",
    "    print(token.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Stavros! It's great to hear that you're passionate about programming. What kind of programming languages or areas of development are you interested in? Web development, mobile app development, artificial intelligence, machine learning, or something else?"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "\n",
    "class LangChainLLM:\n",
    "    def __init__(self, params: dict = None):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        \"\"\"Initialize the LLM with specified model and parameters.\"\"\"\n",
    "        \n",
    "        # Check if model is specified in params, if not set default to llama3.1\n",
    "        if \"model\" not in params:\n",
    "            params[\"model\"] = \"llama3.1\"\n",
    "            print(f\"Model not specified, defaulting to {params['model']}\")\n",
    "\n",
    "        # Define parameters in a dictionary for easier modification\n",
    "        self.llm_params = params\n",
    "        self.llm = ChatOllama(**self.llm_params)\n",
    "\n",
    "        # Initialize memory to store conversation history\n",
    "        self.memory = []\n",
    "        self.system_message = None\n",
    "\n",
    "    def invoke(self, messages):\n",
    "        \"\"\"Send a single request to the LLM and get response.\"\"\"\n",
    "        try:\n",
    "            # Store messages in memory if they're not already there\n",
    "            self._update_memory(messages)\n",
    "\n",
    "            response = self.llm.invoke(messages)\n",
    "\n",
    "            # Add the response to memory\n",
    "            self._add_to_memory(response)\n",
    "\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Error during invocation: {e}\")\n",
    "            return None\n",
    "\n",
    "    def stream(self, messages):\n",
    "        \"\"\"Stream responses from the LLM.\"\"\"\n",
    "        try:\n",
    "            # Store messages in memory before streaming\n",
    "            self._update_memory(messages)\n",
    "\n",
    "            # We'll collect the full response to add to memory after streaming\n",
    "            return self.llm.stream(messages)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during streaming: {e}\")\n",
    "            return None\n",
    "\n",
    "    def stream2(self, messages):\n",
    "        output = \"\"\n",
    "        for token in llm.stream(messages):\n",
    "            output += str(token.content)\n",
    "            yield (token)\n",
    "        message = AIMessage(output)\n",
    "        self._update_memory([message])\n",
    "\n",
    "    def batch(self, message_list):\n",
    "        \"\"\"Process multiple message sets in batch.\"\"\"\n",
    "        try:\n",
    "            # Note: Memory handling for batch operations is more complex\n",
    "            # and might need custom implementation based on use case\n",
    "            return self.llm.batch(message_list)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during batch processing: {e}\")\n",
    "            return None\n",
    "\n",
    "    def generate_prompt(self, system_message, user_message, use_memory=True):\n",
    "        \"\"\"Helper to create properly formatted message list.\"\"\"\n",
    "        # Store the system message for future use\n",
    "        if self.system_message is None:\n",
    "            self.system_message = system_message\n",
    "\n",
    "        if use_memory and self.memory:\n",
    "            # Create a message list with system message and all conversation history\n",
    "            messages = [SystemMessage(system_message)]\n",
    "            messages.extend(self.memory)\n",
    "            messages.append(HumanMessage(user_message))\n",
    "            return messages\n",
    "        else:\n",
    "            # Return just the new messages without history\n",
    "            return [SystemMessage(system_message), HumanMessage(user_message)]\n",
    "\n",
    "    def _update_memory(self, messages):\n",
    "        \"\"\"Update memory with new messages if they're not already stored.\"\"\"\n",
    "        # Extract and store system message if present\n",
    "        if messages and isinstance(messages[0], SystemMessage):\n",
    "            self.system_message = messages[0].content\n",
    "\n",
    "        # Add human and AI messages to memory, skipping system message\n",
    "        for msg in messages:\n",
    "            if not isinstance(msg, SystemMessage) and msg not in self.memory:\n",
    "                self.memory.append(msg)\n",
    "\n",
    "    def _add_to_memory(self, message):\n",
    "        \"\"\"Add a response message to memory.\"\"\"\n",
    "        if message and message not in self.memory:\n",
    "            self.memory.append(message)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        \"\"\"Clear the conversation history.\"\"\"\n",
    "        self.memory = []\n",
    "\n",
    "    def get_memory(self):\n",
    "        \"\"\"Return the current conversation history.\"\"\"\n",
    "        return self.memory\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "llm = LangChainLLM(params={\"model\": \"llama3.1\"})\n",
    "messages = llm.generate_prompt(\n",
    "    \"You are a helpful assistant.\",\n",
    "    \"My name is Stavros and I love programming.\",\n",
    ")\n",
    "for token in llm.stream2(messages):\n",
    "    print(token.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='My name is Stavros and I love programming.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Nice to meet you, Stavros! It's great that you're passionate about programming. What kind of programming languages or areas are you most interested in? Web development, mobile app creation, artificial intelligence, or something else?\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='My name is Stavros and I love programming.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Nice to meet you, Stavros! It's great to hear that you're passionate about programming. What kind of programming languages or areas of development are you interested in? Web development, mobile app development, artificial intelligence, machine learning, or something else?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is the capital of France? Also, what is my name?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Two quick questions!\\n\\nThe capital of France is Paris.\\n\\nAnd... your name is Stavros!', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-03-20T09:35:25.695079Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2846446900, 'load_duration': 30571300, 'prompt_eval_count': 97, 'prompt_eval_duration': 859307600, 'eval_count': 21, 'eval_duration': 1955544900, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-a69e734b-e3de-443c-81c4-cbc20ed5e0dd-0', usage_metadata={'input_tokens': 97, 'output_tokens': 21, 'total_tokens': 118})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(llm.get_memory() + [HumanMessage('What is the capital of France? Also, what is my name?')])\n",
    "llm.get_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, so I need to translate the sentence \"I love programming.\" into German. Let me think about how to approach this.\\n\\nFirst, I know that \"I love\" in English is usually translated as \"Ich liebe\" in German. So that part seems straightforward.\\n\\nNext, \"programming\" is the subject here. In German, the word for programming is \"Programmieren.\" But wait, sometimes people might refer to it as just \"Programmen,\" but I think \"Programmieren\" is more accurate when talking about the activity of programming rather than the noun form.\\n\\nPutting it together, \"Ich liebe Programmieren.\" That makes sense. Let me double-check if there\\'s a better way to phrase it. Maybe using \"to love\" instead of \"I love\"? But in this case, since it\\'s about the person\\'s feelings, \"Ich liebe\" is appropriate.\\n\\nAlso, I should consider the context. If someone says they love programming, it\\'s about their passion for writing code or developing software. So \"Programmieren\" fits well here because it means to program or to write code.\\n\\nI don\\'t think there are any grammatical issues with this translation. It flows naturally and accurately conveys the original sentiment.\\n</think>\\n\\nThe translation of \"I love programming.\" into German is:\\n\\n\"Ich liebe Programmieren.\"\\n\\nThis phrase accurately conveys the passion for coding and software development in German.', additional_kwargs={}, response_metadata={'model': 'deepseek-r1:8b', 'created_at': '2025-03-18T06:40:07.8325519Z', 'done': True, 'done_reason': 'stop', 'total_duration': 31595215300, 'load_duration': 2280331900, 'prompt_eval_count': 18, 'prompt_eval_duration': 569000000, 'eval_count': 286, 'eval_duration': 28744000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-8e9a7adc-3190-4de4-89f5-ac12b6db8481-0', usage_metadata={'input_tokens': 18, 'output_tokens': 286, 'total_tokens': 304})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm.llm\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"German\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'get_user_name',\n",
       "  'args': {'addresses': ['123 Fake St, Boston, MA',\n",
       "    '234 Pretend Boulevard, Houston, TX'],\n",
       "   'user_id': 123},\n",
       "  'id': '35cb4bad-d829-48b2-9099-fde8eba3c192',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_user_name(user_id: int, addresses: List[str])-> str:\n",
    "    \"\"\"Return user name using historical addresses.\n",
    "\n",
    "    Args:\n",
    "        user_id (int): the user ID.\n",
    "        addresses (List[str]): Previous addresses as a list of strings.\n",
    "    \"\"\"\n",
    "    return \"Stavros Pitoglou\"\n",
    "\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0,\n",
    ").bind_tools([get_user_name])\n",
    "\n",
    "result = llm.invoke(\n",
    "    \"Could you bring me the name of the user with id 123? They previously lived at \"\n",
    "    \"123 Fake St in Boston MA and 234 Pretend Boulevard in \"\n",
    "    \"Houston TX. The name should be the output of the get_user_name tool.\"\n",
    ")\n",
    "result.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-03-18T07:11:07.4654946Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4177865400, 'load_duration': 27764300, 'prompt_eval_count': 247, 'prompt_eval_duration': 604000000, 'eval_count': 40, 'eval_duration': 3545000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-187bc9f0-5bd7-43b5-a389-ba57b5699b29-0', tool_calls=[{'name': 'get_user_name', 'args': {'addresses': ['123 Fake St, Boston, MA', '234 Pretend Boulevard, Houston, TX'], 'user_id': 123}, 'id': '35cb4bad-d829-48b2-9099-fde8eba3c192', 'type': 'tool_call'}], usage_metadata={'input_tokens': 247, 'output_tokens': 40, 'total_tokens': 287})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'Return user name using historical addresses.\\n\\nArgs:\\n    user_id (int): the user ID.\\n    addresses (List[str]): Previous addresses as a list of strings.',\n",
       " 'properties': {'user_id': {'title': 'User Id', 'type': 'integer'},\n",
       "  'addresses': {'items': {'type': 'string'},\n",
       "   'title': 'Addresses',\n",
       "   'type': 'array'}},\n",
       " 'required': ['user_id', 'addresses'],\n",
       " 'title': 'get_user_name',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_user_name.args_schema.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "res = search.invoke(\"Obama's first name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The White House, official residence of the president of the United States, '\n",
      " 'in July 2008. The president of the United States is the head of state and '\n",
      " 'head of government of the United States, [1] indirectly elected to a '\n",
      " 'four-year term via the Electoral College. [2] Under the U.S. Constitution, '\n",
      " 'the officeholder leads the executive branch of the federal government and is '\n",
      " 'the commander-in-chief of ... As the head of the government of the United '\n",
      " 'States, the president is arguably the most powerful government official in '\n",
      " 'the world. The president is elected to a four-year term via an electoral '\n",
      " 'college system. Since the Twenty-second Amendment was adopted in 1951, the '\n",
      " 'American presidency has been limited to a maximum of two terms.. Click on a '\n",
      " \"president below to learn more about each presidency ... The Irish Sun, It's \"\n",
      " \"a fake Barack Obama's brother posts forged document he claims is \"\n",
      " \"ex-president's 'Kenyan birth certificate,' March 11, 2017 Salon, Orly Taitz \"\n",
      " 'is at it again , Sept. 4, 2009 Born on August 4, 1961, in Honolulu, Hawaii, '\n",
      " 'Obama is the first president born outside the continental United States. His '\n",
      " \"full name is Barack Hussein Obama II, named after his father. Obama's \"\n",
      " 'mother, Ann Dunham, was from Kansas, while his father, Barack Obama Sr., '\n",
      " 'hailed from Kenya. Timeline of Barack Obama August 4, 1961. Barack Hussein '\n",
      " 'Obama II is born in Honolulu, Hawaii, to Barack Obama Sr., a Kenyan '\n",
      " 'economist, and Ann Dunham, an anthropologist from Kansas. His parents met '\n",
      " 'while attending the University of Hawaii. They divorced when Barack was two '\n",
      " 'years old, leaving Ann to raise him primarily on her own.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "type(res)\n",
    "pprint(str(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"snippet: The former First Lady denied any rift with Barack Obama and praised school counselors in her first public video since speculation about their relationship. Learn about their love story, family and legacy in this article., title: Michelle Obama makes emotional announcement amid speculation over ..., link: https://www.hellomagazine.com/celebrities/812295/michele-obama-makes-emotional-announcement-amid-divorce-speculation/, snippet: Former Presidents Clinton, Obama, Biden and George W. Bush have barely uttered a word about President Trump's actions a month into his second term, to the dismay of Democratic critics who say‚Ä¶, title: Obama, Bush, Clinton, Biden staying silent on Trump dismays some Democrats, link: https://thehill.com/homenews/administration/5153858-former-presidents-trump-actions/, snippet: A viral claim suggests that Trump's allies proposed a constitutional amendment that would allow presidents to have a third term, including Obama. But the amendment only applies to presidents who served nonconsecutive terms, like Trump., title: Obama barred from proposal to allow presidents third term | Fact check, link: https://www.usatoday.com/story/news/factcheck/2025/01/29/obama-trump-president-third-term-fact-check/77998416007/, snippet: Learn about the life and career of Barack Obama, the 44th president of the United States, who rose from humble beginnings to become a best-selling author, a U.S. senator, and a Nobel Peace Prize laureate. Explore his achievements, challenges, and legacy in this comprehensive article from Britannica., title: Barack Obama - 44th President, Political Career, Legacy | Britannica, link: https://www.britannica.com/biography/Barack-Obama/Politics-and-ascent-to-the-presidency\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "\n",
    "search = DuckDuckGoSearchResults()\n",
    "\n",
    "search.invoke(\"Obama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'snippet': 'Learn about the life and achievements of Barack Obama, the 44th president of the United States and the first African American to hold that office. Find out his biography, awards, books, family, and more.',\n",
       "  'title': 'Barack Obama Facts | Britannica',\n",
       "  'link': 'https://www.britannica.com/facts/Barack-Obama'},\n",
       " {'snippet': 'Obama, in his remarks, insisted that he is \"convinced that if we want democracy as we understand it to survive,\" people must work for a renewed dedication to pluralist principles.',\n",
       "  'title': \"Obama, in 1st remarks since election, says 'a line has been ... - Fox News\",\n",
       "  'link': 'https://www.foxnews.com/politics/obama-first-remarks-since-election-says-a-line-has-been-crossed-one-side-makes-certain-moves'},\n",
       " {'snippet': 'Learn about the life and career of Barack Obama, the 44th president of the United States, who rose from humble beginnings to become a best-selling author, a U.S. senator, and a Nobel Peace Prize laureate. Explore his achievements, challenges, and legacy in this comprehensive article from Britannica.',\n",
       "  'title': 'Barack Obama - 44th President, Political Career, Legacy | Britannica',\n",
       "  'link': 'https://www.britannica.com/biography/Barack-Obama/Politics-and-ascent-to-the-presidency'},\n",
       " {'snippet': 'Through her production company, Higher Ground, Mrs. Obama, along with her brother, Craig Robinson, will interview celebrities and offer advice on various topics. By Jessica Testa Ask any prominent ...',\n",
       "  'title': \"Michelle Obama Is Hosting Video Podcast 'IMO' With Her Brother, Craig ...\",\n",
       "  'link': 'https://www.nytimes.com/2025/03/10/business/media/michelle-obama-video-podcast.html'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = DuckDuckGoSearchResults(output_format=\"list\")\n",
    "\n",
    "search.invoke(\"Obama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-20 14:32:03.431\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcssallmlib.implementations.llm_translation\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mInitialized TranslationChain with source language: English\u001b[0m\n",
      "\u001b[32m2025-03-20 14:32:03.432\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcssallmlib.implementations.llm_translation\u001b[0m:\u001b[36mtranslate\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mStarting translation from English to German\u001b[0m\n",
      "\u001b[32m2025-03-20 14:32:03.433\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcssallmlib.implementations.llm_translation\u001b[0m:\u001b[36mtranslate\u001b[0m:\u001b[36m54\u001b[0m - \u001b[34m\u001b[1mInput text: I love programming.\u001b[0m\n",
      "\u001b[32m2025-03-20 14:32:13.282\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcssallmlib.implementations.llm_translation\u001b[0m:\u001b[36mtranslate\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mSuccessfully translated text to German\u001b[0m\n",
      "\u001b[32m2025-03-20 14:32:13.283\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcssallmlib.implementations.llm_translation\u001b[0m:\u001b[36mtranslate\u001b[0m:\u001b[36m59\u001b[0m - \u001b[34m\u001b[1mTranslated text: The translation of \"I love programming\" into German is:\n",
      "\n",
      "\"Ich liebe Programmieren.\"\n",
      "\n",
      "Here's a breakd...\u001b[0m\n",
      "\u001b[32m2025-03-20 14:32:13.284\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcssallmlib.implementations.llm_translation\u001b[0m:\u001b[36mbatch_translate\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mStarting batch translation of 3 texts from English to Spanish\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: The translation of \"I love programming\" into German is:\n",
      "\n",
      "\"Ich liebe Programmieren.\"\n",
      "\n",
      "Here's a breakdown of the translation:\n",
      "\n",
      "* \"Ich liebe\" means \"I love\"\n",
      "* \"Programmieren\" means \"programming\"\n",
      "\n",
      "Note that in German, the verb \"lieben\" (to love) typically takes the dative case when referring to an activity or profession, which is why it's written as \"Programmieren\" instead of just \"Programmierung\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-20 14:32:36.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcssallmlib.implementations.llm_translation\u001b[0m:\u001b[36mbatch_translate\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mSuccessfully completed batch translation of 3 texts to Spanish\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch translations:\n",
      "Hello world! -> The translation of \"Hello world!\" into Spanish is:\n",
      "\n",
      "\"Hola mundo!\"\n",
      "\n",
      "(Note: This is a common phrase used in programming, especially when introducing a new language or environment. It's not typically used as a greeting in everyday conversation.)\n",
      "How are you? -> The translation of \"How are you?\" into Spanish is:\n",
      "\n",
      "**¬øC√≥mo est√°s?**\n",
      "\n",
      "(Note: This is a formal way to ask how someone is doing. If you want to use an informal tone, the translation would be **¬øC√≥mo est√°s?** becomes **¬øQu√© onda?** or simply **¬øC√≥mo est√°s?** in some Latin American countries.)\n",
      "Good morning! -> The translation of \"Good morning!\" into Spanish is:\n",
      "\n",
      "**Buenos d√≠as!**\n",
      "\n",
      "(Note: In some Latin American countries, it's more common to say \"Buenos d√≠as\" until around 11 am or noon, after which you would switch to \"Buenas tardes\" for the afternoon and \"Buenas noches\" for the evening. However, in Spain, it's generally acceptable to use \"Buenos d√≠as\" throughout the morning.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from cssallmlib.implementations.llm_translation import TranslationChain\n",
    "\n",
    "# Initialize the model and translation chain\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "translator = TranslationChain(llm)\n",
    "\n",
    "# Single translation\n",
    "result = translator.translate(\"I love programming.\", \"German\")\n",
    "print(f\"Translation: {result}\")\n",
    "\n",
    "# Batch translation\n",
    "texts = [\"Hello world!\", \"How are you?\", \"Good morning!\"]\n",
    "results = translator.batch_translate(texts, \"Spanish\")\n",
    "print(\"\\nBatch translations:\")\n",
    "for original, translated in zip(texts, results):\n",
    "    print(f\"{original} -> {translated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmlib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
